{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzUCCsNZV3A2",
        "outputId": "e3289b19-ade0-44c4-a38b-645bfe56b10b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Metrics:\n",
            "\n",
            "Email Extraction:\n",
            "precision: 0.440\n",
            "recall: 0.452\n",
            "f1: 0.444\n",
            "accuracy: 0.452\n",
            "\n",
            "Phone Extraction:\n",
            "precision: 0.083\n",
            "recall: 0.103\n",
            "f1: 0.090\n",
            "accuracy: 0.103\n",
            "\n",
            "Name Extraction:\n",
            "precision: 0.354\n",
            "recall: 0.487\n",
            "f1: 0.396\n",
            "accuracy: 0.487\n",
            "\n",
            "Order_Id Extraction:\n",
            "precision: 0.585\n",
            "recall: 0.851\n",
            "f1: 0.674\n",
            "accuracy: 0.851\n",
            "\n",
            "Time per Conversation Parsing: 1.765 seconds\n",
            "\n",
            "Overall Accuracy: 0.429\n",
            "\n",
            "Detailed Results (first 5 conversations):\n",
            "\n",
            "Conversation ID: 9784\n",
            "Extracted: {'email': [], 'phone': ['4905628193'], 'name': ['Sanya Afzal'], 'order_id': ['BCMZTYKLKU', '4905628193']}\n",
            "Ground Truth: {'email': '', 'phone': '(320) 090-2639', 'name': 'Sanya Afzal', 'order_id': '4905628193'}\n",
            "Metrics: {'email': {'precision': 0, 'recall': 0, 'f1': 0, 'support': 0, 'accuracy': 0}, 'phone': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'name': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 1, 'accuracy': 1.0}, 'order_id': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'support': 1, 'accuracy': 1.0}}\n",
            "\n",
            "Conversation ID: 4809\n",
            "Extracted: {'email': [], 'phone': ['4772985976'], 'name': ['Chloe Zhang'], 'order_id': ['4772985976', 'V7XH6YJDBA']}\n",
            "Ground Truth: {'email': 'chloez01@email.com', 'phone': '(628) 223-9981', 'name': 'Chloe Zhang', 'order_id': '4772985976'}\n",
            "Metrics: {'email': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'phone': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'name': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 1, 'accuracy': 1.0}, 'order_id': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'support': 1, 'accuracy': 1.0}}\n",
            "\n",
            "Conversation ID: 6753\n",
            "Extracted: {'email': [], 'phone': [], 'name': ['Joyce'], 'order_id': []}\n",
            "Ground Truth: {'email': 'joycew65@email.com', 'phone': '(772) 468-5380', 'name': 'Joyce Wu', 'order_id': '6459643450'}\n",
            "Metrics: {'email': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'phone': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'name': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'order_id': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}}\n",
            "\n",
            "Conversation ID: 1036\n",
            "Extracted: {'email': [], 'phone': ['8743755824'], 'name': ['Chloe Zhang'], 'order_id': ['TSR5UFXV7B', '8743755824']}\n",
            "Ground Truth: {'email': 'cz278656@email.com', 'phone': '(305) 060-4365', 'name': 'Chloe Zhang', 'order_id': '8743755824'}\n",
            "Metrics: {'email': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'phone': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'name': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'support': 1, 'accuracy': 1.0}, 'order_id': {'precision': 0.5, 'recall': 1.0, 'f1': 0.6666666666666666, 'support': 1, 'accuracy': 1.0}}\n",
            "\n",
            "Conversation ID: 7086\n",
            "Extracted: {'email': [], 'phone': ['1234567912'], 'name': [], 'order_id': ['123456791234']}\n",
            "Ground Truth: {'email': 'jwu35512@email.com', 'phone': '(436) 240-6458', 'name': 'Joyce Wu', 'order_id': ''}\n",
            "Metrics: {'email': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'phone': {'precision': 0.0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'name': {'precision': 0, 'recall': 0.0, 'f1': 0, 'support': 1, 'accuracy': 0.0}, 'order_id': {'precision': 0.0, 'recall': 0, 'f1': 0, 'support': 0, 'accuracy': 0}}\n"
          ]
        }
      ],
      "source": [
        "#import statements\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
        "\n",
        "#functions for each extraction\n",
        "def extract_email(text: str) -> List[str]:\n",
        "    #email\n",
        "    \"\"\"Extract email addresses from text using regex.\"\"\"\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    return re.findall(email_pattern, text)\n",
        "\n",
        "def extract_phone(text: str) -> List[str]:\n",
        "    #phone\n",
        "    \"\"\"Extract phone numbers from text using regex.\"\"\"\n",
        "    phone_pattern = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
        "    return re.findall(phone_pattern, text)\n",
        "\n",
        "def extract_name(text: str) -> List[str]:\n",
        "    #name\n",
        "    \"\"\"Extract names from text using BERT NER.\"\"\"\n",
        "    entities = ner_pipeline(text)\n",
        "    return [entity['word'] for entity in entities if entity['entity_group'] == 'PER']\n",
        "\n",
        "def extract_order_id(text: str) -> List[str]:\n",
        "    #order_id\n",
        "    \"\"\"Extract order IDs from text using regex.\"\"\"\n",
        "    order_id_pattern = r'\\b[0-9A-Z]{8,}\\b'  # Adjust pattern based on order ID format\n",
        "    return re.findall(order_id_pattern, text)\n",
        "\n",
        "def get_ground_truth(scenario: Dict) -> Dict[str, str]:\n",
        "    #compare to ground truth values\n",
        "    \"\"\"Extract ground truth values from the scenario data.\"\"\"\n",
        "    personal = scenario.get('personal', {})\n",
        "    order = scenario.get('order', {})\n",
        "    return {\n",
        "        'email': personal.get('email', ''),\n",
        "        'phone': personal.get('phone', ''),\n",
        "        'name': personal.get('customer_name', '').title(),\n",
        "        'order_id': order.get('order_id', '')\n",
        "    }\n",
        "\n",
        "def extract_entities_from_conversation(conversation: List[List[str]]) -> Dict[str, List[str]]:\n",
        "    #extract features based on definitions above\n",
        "    \"\"\"Extract entities from all messages in a conversation.\"\"\"\n",
        "    entities = {\n",
        "        'email': set(),\n",
        "        'phone': set(),\n",
        "        'name': set(),\n",
        "        'order_id': set()\n",
        "    }\n",
        "    #combine all messages into a single string\n",
        "    text = ' '.join(message for _, message in conversation)\n",
        "\n",
        "    # xxtract entities\n",
        "    entities['email'].update(extract_email(text))\n",
        "    entities['phone'].update(extract_phone(text))\n",
        "    entities['name'].update(extract_name(text))\n",
        "    entities['order_id'].update(extract_order_id(text))\n",
        "\n",
        "    return {k: list(v) for k, v in entities.items()}\n",
        "\n",
        "  #calculate metrics for each\n",
        "def evaluate_extraction(predicted: Dict[str, List[str]], actual: Dict[str, str]) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"Calculate precision, recall, and F1 score for each entity type.\"\"\"\n",
        "    metrics = {}\n",
        "    for entity_type in ['email', 'phone', 'name', 'order_id']:\n",
        "        # convert to sets for comparison\n",
        "        true_entities = set([actual[entity_type]]) if actual[entity_type] else set()\n",
        "        predicted_entities = set(predicted[entity_type])\n",
        "\n",
        "        # calculate true positives, false positives, and false negatives\n",
        "        tp = len(true_entities & predicted_entities)\n",
        "        fp = len(predicted_entities - true_entities)\n",
        "        fn = len(true_entities - predicted_entities)\n",
        "\n",
        "        # compute precision, recall, F1, and accuracy\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        accuracy = tp / len(true_entities) if len(true_entities) > 0 else 0\n",
        "\n",
        "        #make list for metrics\n",
        "        metrics[entity_type] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'support': len(true_entities),\n",
        "            'accuracy': accuracy\n",
        "        }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "#go through each convo and calculate\n",
        "def process_conversations(data: Dict) -> Tuple[Dict[str, Dict[str, float]], List[Dict], float, float]:\n",
        "    \"\"\"Process all conversations, calculate overall metrics, measure parsing time, and overall accuracy.\"\"\"\n",
        "    all_metrics = []\n",
        "    overall_metrics = {\n",
        "        'email': {'precision': 0, 'recall': 0, 'f1': 0, 'support': 0, 'accuracy': 0},\n",
        "        'phone': {'precision': 0, 'recall': 0, 'f1': 0, 'support': 0, 'accuracy': 0},\n",
        "        'name': {'precision': 0, 'recall': 0, 'f1': 0, 'support': 0, 'accuracy': 0},\n",
        "        'order_id': {'precision': 0, 'recall': 0, 'f1': 0, 'support': 0, 'accuracy': 0}\n",
        "    }\n",
        "    total_time = 0  # total time for processing all conversations\n",
        "    correct_fields = 0  # total correct fields across all conversations\n",
        "    total_fields = 0  # total fields that needed to be extracted\n",
        "\n",
        "    #in training\n",
        "    for conversation_data in data['train']:\n",
        "        #start timing the conversation processing\n",
        "        start_time = time.time()\n",
        "\n",
        "        #extract ground truth\n",
        "        ground_truth = get_ground_truth(conversation_data['scenario'])\n",
        "\n",
        "        #extract entities from conversation\n",
        "        extracted_entities = extract_entities_from_conversation(conversation_data['original'])\n",
        "\n",
        "        #calculate metrics\n",
        "        metrics = evaluate_extraction(extracted_entities, ground_truth)\n",
        "\n",
        "        #count correctly extracted fields for overall accuracy\n",
        "        for entity_type in ['email', 'phone', 'name', 'order_id']:\n",
        "            if ground_truth[entity_type] and ground_truth[entity_type] in extracted_entities[entity_type]:\n",
        "                correct_fields += 1\n",
        "            if ground_truth[entity_type]:\n",
        "                total_fields += 1\n",
        "\n",
        "        #end timing\n",
        "        end_time = time.time()\n",
        "        total_time += (end_time - start_time)\n",
        "\n",
        "        #store results\n",
        "        result = {\n",
        "            'convo_id': conversation_data['convo_id'],\n",
        "            'extracted': extracted_entities,\n",
        "            'ground_truth': ground_truth,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "        #all results stored in all_metrics list\n",
        "        all_metrics.append(result)\n",
        "\n",
        "        #update overall metrics\n",
        "        for entity_type in overall_metrics:\n",
        "            for metric in ['precision', 'recall', 'f1', 'accuracy']:\n",
        "                overall_metrics[entity_type][metric] += metrics[entity_type][metric] * metrics[entity_type]['support']\n",
        "            overall_metrics[entity_type]['support'] += metrics[entity_type]['support']\n",
        "\n",
        "    # calculate weighted averages\n",
        "    for entity_type in overall_metrics:\n",
        "        if overall_metrics[entity_type]['support'] > 0:\n",
        "            for metric in ['precision', 'recall', 'f1', 'accuracy']:\n",
        "                overall_metrics[entity_type][metric] /= overall_metrics[entity_type]['support']\n",
        "\n",
        "    #calculate average time per conversation\n",
        "    avg_time_per_conversation = total_time / len(data['train']) if len(data['train']) > 0 else 0\n",
        "\n",
        "    #calculate overall accuracy\n",
        "    overall_accuracy = correct_fields / total_fields if total_fields > 0 else 0\n",
        "\n",
        "    return overall_metrics, all_metrics, avg_time_per_conversation, overall_accuracy\n",
        "\n",
        "#example usage\n",
        "if __name__ == \"__main__\":\n",
        "    #load your JSON data\n",
        "    with open('/content/8_1_1_small.json.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    #process all conversations\n",
        "    overall_metrics, detailed_results, avg_time_per_conversation, overall_accuracy = process_conversations(data)\n",
        "\n",
        "    #print overall metrics\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    for entity_type, metrics in overall_metrics.items():\n",
        "        print(f\"\\n{entity_type.title()} Extraction:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            if metric_name != 'support':\n",
        "                print(f\"{metric_name}: {value:.3f}\")\n",
        "\n",
        "    #print average time per conversation\n",
        "    print(f\"\\nTime per Conversation Parsing: {avg_time_per_conversation:.3f} seconds\")\n",
        "\n",
        "    #print overall accuracy\n",
        "    print(f\"\\nOverall Accuracy: {overall_accuracy:.3f}\")\n",
        "\n",
        "    #print detailed results for first few conversations\n",
        "    print(\"\\nDetailed Results (first 5 conversations):\")\n",
        "    for result in detailed_results[:5]:\n",
        "        print(f\"\\nConversation ID: {result['convo_id']}\")\n",
        "        print(\"Extracted:\", result['extracted'])\n",
        "        print(\"Ground Truth:\", result['ground_truth'])\n",
        "        print(\"Metrics:\", result['metrics'])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
