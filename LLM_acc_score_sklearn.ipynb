{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oQo2nxJWfzj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_data(json_path):\n",
        "    with open(json_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def extract_true_vals(data):\n",
        "    \"\"\"\n",
        "    Extract the true values (customer_name, email, phone) from the JSON data.\n",
        "    Returns list of dicts that are true values from the conversation\n",
        "    \"\"\"\n",
        "    true_vals = []\n",
        "    for i in data:\n",
        "        scenario = i.get('scenario', {})\n",
        "        personal_info = scenario.get('personal', {})\n",
        "        true_vals.append({\n",
        "            'customer_name': personal_info.get('customer_name', '').lower(),\n",
        "            'email': personal_info.get('email', '').lower(),\n",
        "            'phone': personal_info.get('phone', '').lower()\n",
        "        })\n",
        "    return true_vals\n",
        "\n",
        "def convert_to_comparable_format(vals):\n",
        "    \"\"\"\n",
        "    Convert a list of dictionaries to a list of tuples for comparison.\n",
        "    \"\"\"\n",
        "    return [(val['customer_name'], val['email'], val['phone']) for val in vals]\n",
        "\n",
        "def test_llm_predictions(json_path, llm_predictions):\n",
        "    \"\"\"\n",
        "    Run all methods.\n",
        "    Arg llm_predictions is a list of dicts.\n",
        "    Returns float accuracy score of LLM using sklearn's accuracy_score for each conversation.\n",
        "    \"\"\"\n",
        "    data = load_json_data(json_path)\n",
        "    true_vals = extract_true_vals(data)\n",
        "\n",
        "    # Convert true values and predictions to comparable formats\n",
        "    y_true = convert_to_comparable_format(true_vals)\n",
        "    y_pred = convert_to_comparable_format(llm_predictions)\n",
        "\n",
        "    # Calculate accuracy for each conversation (entire tuple must match)\n",
        "    conversation_accuracy = [1 if true == pred else 0 for true, pred in zip(y_true, y_pred)]\n",
        "\n",
        "    acc_score = accuracy_score([1] * len(conversation_accuracy), conversation_accuracy)\n",
        "\n",
        "    return acc_score * 100"
      ],
      "metadata": {
        "id": "ukZcvegnWhSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to main json file within your specific Google Drive, please mount GDrive\n",
        "json_file_path = '/content/drive/MyDrive/abcd_sample.json'\n",
        "\n",
        "# LLM predictions (used sample, for testing reasons)\n",
        "# Paste your results here?\n",
        "llm_predictions = [\n",
        "    {'customer_name': 'crystal minh', 'email': 'cminh730@email.com', 'phone': '(977) 625-2661'},\n",
        "    {'customer_name': 'alessandro phoenix', 'email': 'enacjmac@gmail.com', 'phone': '(727) 760-7806'}, # incorrect email example\n",
        "    {'customer_name': 'joyce wu', 'email': '', 'phone': '(859) 787-9085'},\n",
        "    ]\n",
        "\n",
        "# Test LLM predictions against the ground truth\n",
        "acc_score = test_llm_predictions(json_file_path, llm_predictions)\n",
        "\n",
        "print(f\"LLM Prediction Accuracy: {acc_score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LC-rRlWlW0",
        "outputId": "4e47f0a1-13b3-4ce3-bc31-c2380ca677ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Prediction Accuracy: 66.66666666666666%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQsEYE69XW-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}